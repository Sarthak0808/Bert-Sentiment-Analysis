{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\ndf = pd.read_csv(\"../input/dataset/train.tsv\",sep=\"\\t\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T17:32:33.771390Z","iopub.execute_input":"2021-07-20T17:32:33.771702Z","iopub.status.idle":"2021-07-20T17:32:34.117222Z","shell.execute_reply.started":"2021-07-20T17:32:33.771675Z","shell.execute_reply":"2021-07-20T17:32:34.116443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-20T17:32:44.234169Z","iopub.execute_input":"2021-07-20T17:32:44.234523Z","iopub.status.idle":"2021-07-20T17:32:44.242085Z","shell.execute_reply.started":"2021-07-20T17:32:44.234491Z","shell.execute_reply":"2021-07-20T17:32:44.241103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop_duplicates(\n    subset=['SentenceId'],\n    keep='first'\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T17:32:46.176008Z","iopub.execute_input":"2021-07-20T17:32:46.176358Z","iopub.status.idle":"2021-07-20T17:32:46.193618Z","shell.execute_reply.started":"2021-07-20T17:32:46.176329Z","shell.execute_reply":"2021-07-20T17:32:46.192705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set array dimensions\nseq_len = 256\nnum_samples = len(df)\n\n# initialize empty zero arrays\nXids = np.zeros((num_samples, seq_len))\nXmask = np.zeros((num_samples, seq_len))\n\n# check shape\nprint(Xids.shape)\nprint(Xmask.shape)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-20T17:32:52.171713Z","iopub.execute_input":"2021-07-20T17:32:52.172113Z","iopub.status.idle":"2021-07-20T17:32:52.180276Z","shell.execute_reply.started":"2021-07-20T17:32:52.172080Z","shell.execute_reply":"2021-07-20T17:32:52.178947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizer\n# initialize tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n\nfor i, phrase in enumerate(df['Phrase']):\n    tokens = tokenizer.encode_plus(phrase, max_length=seq_len, truncation=True,\n                                   padding='max_length', add_special_tokens=True,\n                                   return_tensors='tf')\n    # assign tokenized outputs to respective rows in numpy arrays\n    Xids[i, :] = tokens['input_ids']\n    Xmask[i, :] = tokens['attention_mask']","metadata":{"execution":{"iopub.status.busy":"2021-07-20T17:32:55.448755Z","iopub.execute_input":"2021-07-20T17:32:55.449108Z","iopub.status.idle":"2021-07-20T17:33:16.531934Z","shell.execute_reply.started":"2021-07-20T17:32:55.449074Z","shell.execute_reply":"2021-07-20T17:33:16.531091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# first extract sentiment column\narr = df['Sentiment'].values\n\n# we then initialize the zero array\nlabels = np.zeros((num_samples, arr.max()+1))\n\n# set relevant index for each row to 1 (one-hot encode)\nlabels[np.arange(num_samples), arr] = 1","metadata":{"execution":{"iopub.status.busy":"2021-07-20T17:33:19.441370Z","iopub.execute_input":"2021-07-20T17:33:19.441729Z","iopub.status.idle":"2021-07-20T17:33:19.447551Z","shell.execute_reply.started":"2021-07-20T17:33:19.441698Z","shell.execute_reply":"2021-07-20T17:33:19.446453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n# create the dataset object\ndataset = tf.data.Dataset.from_tensor_slices((Xids, Xmask, labels))\n\ndef map_func(input_ids, masks, labels):\n    # we convert our three-item tuple into a two-item tuple where the input item is a dictionary\n    return {'input_ids': input_ids, 'attention_mask': masks}, labels\n\n# then we use the dataset map method to apply this transformation\ndataset = dataset.map(map_func)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T17:33:23.753390Z","iopub.execute_input":"2021-07-20T17:33:23.753706Z","iopub.status.idle":"2021-07-20T17:33:23.848828Z","shell.execute_reply.started":"2021-07-20T17:33:23.753675Z","shell.execute_reply":"2021-07-20T17:33:23.847939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we will split into batches of 16\nbatch_size = 16\n\n# shuffle and batch - dropping any remaining samples that don't cleanly\n# fit into a batch of 16\ndataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T17:33:27.752373Z","iopub.execute_input":"2021-07-20T17:33:27.752696Z","iopub.status.idle":"2021-07-20T17:33:27.762022Z","shell.execute_reply.started":"2021-07-20T17:33:27.752667Z","shell.execute_reply":"2021-07-20T17:33:27.761130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set split size (90% training data) and calculate training set size\nsplit = 0.9\nsize = int((Xids.shape[0]/batch_size)*split)\n\n# get training and validation sets\ntrain_ds = dataset.take(size)\nval_ds = dataset.skip(size)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T17:33:29.298862Z","iopub.execute_input":"2021-07-20T17:33:29.299179Z","iopub.status.idle":"2021-07-20T17:33:29.305012Z","shell.execute_reply.started":"2021-07-20T17:33:29.299150Z","shell.execute_reply":"2021-07-20T17:33:29.304186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# AutoModel for PyTorch, TFAutoModel for TensorFlow\nfrom transformers import TFAutoModel\n\nbert = TFAutoModel.from_pretrained('bert-base-cased')","metadata":{"execution":{"iopub.status.busy":"2021-07-20T17:33:32.608306Z","iopub.execute_input":"2021-07-20T17:33:32.608638Z","iopub.status.idle":"2021-07-20T17:34:09.866696Z","shell.execute_reply.started":"2021-07-20T17:33:32.608609Z","shell.execute_reply":"2021-07-20T17:34:09.865687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# two input layers, we ensure layer name variables match to dictionary keys in TF dataset\ninput_ids = tf.keras.layers.Input(shape=(256,), name='input_ids', dtype='int32')\nmask = tf.keras.layers.Input(shape=(256,), name='attention_mask', dtype='int32')\n\n# we access the transformer model within our bert object using the bert attribute (eg bert.bert instead of bert)\nembeddings = bert.bert(input_ids, attention_mask=mask)[1]  # access pooled activations with [1]\n\n# convert bert embeddings into 5 output classes\nx = tf.keras.layers.Dense(512, activation='relu')(embeddings)\ny = tf.keras.layers.Dense(5, activation='softmax', name='outputs')(x)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T17:34:34.257000Z","iopub.execute_input":"2021-07-20T17:34:34.257385Z","iopub.status.idle":"2021-07-20T17:34:38.590641Z","shell.execute_reply.started":"2021-07-20T17:34:34.257353Z","shell.execute_reply":"2021-07-20T17:34:38.589702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initialize model\nmodel = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-20T17:34:42.458311Z","iopub.execute_input":"2021-07-20T17:34:42.458695Z","iopub.status.idle":"2021-07-20T17:34:42.485038Z","shell.execute_reply.started":"2021-07-20T17:34:42.458661Z","shell.execute_reply":"2021-07-20T17:34:42.484121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\noptimizer = tf.keras.optimizers.Adam(lr=1e-5, decay=1e-6)\nloss = tf.keras.losses.CategoricalCrossentropy()\nacc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n\nmodel.compile(optimizer=optimizer, loss=loss, metrics=[acc])","metadata":{"execution":{"iopub.status.busy":"2021-07-20T17:34:46.304004Z","iopub.execute_input":"2021-07-20T17:34:46.304377Z","iopub.status.idle":"2021-07-20T17:34:46.337599Z","shell.execute_reply.started":"2021-07-20T17:34:46.304344Z","shell.execute_reply":"2021-07-20T17:34:46.336665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=4\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T17:34:50.799988Z","iopub.execute_input":"2021-07-20T17:34:50.800330Z","iopub.status.idle":"2021-07-20T17:50:51.999317Z","shell.execute_reply.started":"2021-07-20T17:34:50.800298Z","shell.execute_reply":"2021-07-20T17:50:51.998466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('sentiment_model')","metadata":{"execution":{"iopub.status.busy":"2021-07-20T17:50:59.811212Z","iopub.execute_input":"2021-07-20T17:50:59.811556Z","iopub.status.idle":"2021-07-20T17:51:34.779407Z","shell.execute_reply.started":"2021-07-20T17:50:59.811527Z","shell.execute_reply":"2021-07-20T17:51:34.778486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.models.load_model('sentiment_model')","metadata":{"execution":{"iopub.status.busy":"2021-07-20T17:29:57.985731Z","iopub.execute_input":"2021-07-20T17:29:57.986122Z","iopub.status.idle":"2021-07-20T17:30:12.469347Z","shell.execute_reply.started":"2021-07-20T17:29:57.986086Z","shell.execute_reply":"2021-07-20T17:30:12.468331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initialize tokenizer from transformers\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n\ndef prep_data(text):\n    # tokenize to get input IDs and attention mask tensors\n    tokens = tokenizer.encode_plus(text, max_length=256,\n                                   truncation=True, padding='max_length',\n                                   add_special_tokens=True, return_token_type_ids=False,\n                                   return_tensors='tf')\n    # tokenizer returns int32 tensors, we need to return float64, so we use tf.cast\n    return {'input_ids': tf.cast(tokens['input_ids'], tf.float64),\n            'attention_mask': tf.cast(tokens['attention_mask'], tf.float64)}","metadata":{"execution":{"iopub.status.busy":"2021-07-20T17:30:12.471532Z","iopub.execute_input":"2021-07-20T17:30:12.471917Z","iopub.status.idle":"2021-07-20T17:30:12.862422Z","shell.execute_reply.started":"2021-07-20T17:30:12.471877Z","shell.execute_reply":"2021-07-20T17:30:12.861508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"in_tensor_negative = prep_data(\"The movie was awful\")\nin_tensor_positive = prep_data(\"The movie was brilliant\")\n\nprobs_negative = model.predict(in_tensor_negative)[0]\nprobs_positive = model.predict(in_tensor_positive)[0]\n\nprint(probs_negative)\nprint(probs_positive)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T17:30:12.863869Z","iopub.execute_input":"2021-07-20T17:30:12.864252Z","iopub.status.idle":"2021-07-20T17:30:15.658120Z","shell.execute_reply.started":"2021-07-20T17:30:12.864192Z","shell.execute_reply":"2021-07-20T17:30:15.657258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nprint(\"Negative:\", np.argmax(probs_negative))\nprint(\"Positive:\", np.argmax(probs_positive))","metadata":{"execution":{"iopub.status.busy":"2021-07-20T17:31:45.804915Z","iopub.execute_input":"2021-07-20T17:31:45.805259Z","iopub.status.idle":"2021-07-20T17:31:45.820089Z","shell.execute_reply.started":"2021-07-20T17:31:45.805224Z","shell.execute_reply":"2021-07-20T17:31:45.818549Z"},"trusted":true},"execution_count":null,"outputs":[]}]}